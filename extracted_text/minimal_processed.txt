# Documents processed with minimal strategy



========================================
Document: 01 - Introduction & Getting Started.pdf
========================================

DS 4300 Large Scale Information Storage and Retrieval Mark Fontenot, PhD Northeastern University Hi! 󰗞 -Mark Fontenot, PhD - Ofﬁce: 353 Meserve Hall - Ofﬁce Hours: - M & Th 1:30 - 3:00 pm (If those times don’t work, just DM me on Slack to set up an alternate time!) - Usually, very available on Slack… so just DM me. -m.fontenot@northeastern.edu Teaching Assistants 3 Iker Acosta Venegas Dallon Archibald Nathan Cheung Aryan Jain Abhishek Kumar Eddy Liu Sevinch Noori Junxiang Lin Where do I ﬁnd … ? -Course materials (Notes, Assignments, etc): https://markfontenot.net/teaching/ds4300/25s-ds4300/ -Assignment submissions (and grades): GradeScope -Q & A Platform is CampusWire -Quick DMs and Announcements will be on Slack What’s this class about? -By the end of this class, you should - Understand the efﬁciency-related concepts (including limitations) of RDBMSs - Understand data replication and distribution effects on typical DB usage scenarios - Understand the use cases for and data models of various NoSQL database systems, including storing and retrieving data. Data models include document-based, key-value stores, graph based among others. - Access and implement data engineering and big-data-related AWS services Course Deliverables and Evaluation Assignments -Homeworks and Practicals - Usually due Tuesday Nights at 11:59 unless otherwise stated - 3% Bonus for submitting 48 hours early. (No… you can’t get > 3% for submitting >48 hours early) - No Late Submissions accepted! - But… life happens… So everyone gets 1 free, no-questions-asked 48 hour extension. - DM Dr. Fontenot on Slack sometime before the original deadline requesting to use your extension. Assignments -Submissions will be via GradeScope and/or GitHub (unless directed otherwise) - Only submit PDFs unless otherwise instructed. - If only submitting a PDF, be sure to associate questions in gradescope with the correct page in your PDF. - Failure to do so may result in a grade of 0 on the assignment. -All regrade requests must be submitted within 48 hours of grades being released on GradeScope. No Exceptions. Midterm Monday, March 17 Mark it in your calendars now! Final Grade Breakdown -Homeworks (5) 30% -Practicals (2) 20% -Midterm 20% -Semester Project 30% Reference Materials Primary Resources 11 O’Reilly Playlist Other books are in the playlist. I will add additional materials to the playlist or webpage as the semester progresses. Tentative List of Topics -Thinking about data storage and retrieval at the data structures level -How far can we get with the relational model? -NoSQL Databases - Document Databases (Mongo) - Graph Databases (Neo4j) - Key/Value Databases - Maybe Vector Databases -Data Distribution and Replication -Distributed SQL DBs & Apache Spark/SparkSQL -Big Data Tools and Services on AWS Tools You Will Need to Install on your Laptop -Docker Desktop -Anaconda or Miniconda Python - You’re welcome to use another distro, but you’re responsible for ﬁxing it if something doesn’t work (dependency conﬂicts, etc.) -A Database Access tool like Datagrip or DBeaver -VS Code set up for Python Development - See > here < for more info about VSCode, Python, and Anaconda -Ability to interact with git and GitHub through terminal or GUI app. Topics to Review over the Next Few Days - Shell/cmd Prompt/PowerShell CLI - Windows - if you want a Unix terminal: WSL2 or zsh on Windows - navigating the ﬁle system - running commands like pip, conda, python, etc - command line args - Docker & Docker Compose - Basics of Dockerﬁles and docker-compose.yaml ﬁles - port mapping - setting up volumes & mapping between host and guest OS Is your Python Rusty or Haven’t Done a ton with it? - Python Crash Course by Net Ninja on YT - On O’Reilly (See Python section of class playlist): - Python - Object-Oriented Programming Video Course by Simon Sez IT - E. Matthes - Python Crash Course, 3rd Edition - No Starch Press (not related to the YT video playlist listed above) Expectations -Conduct yourself respectfully -Don’t distract your classmates from learning -Don’t cheat!! - Do your own work unless group assignment - Discussing problems is encouraged, but you must formulate your own solutions - See Syllabus for details! Let’s GOOO!



========================================
Document: 02 - Foundations.pdf
========================================

DS 4300 Large Scale Information Storage and Retrieval Foundations Mark Fontenot, PhD Northeastern University Searching -Searching is the most common operation performed by a database system -In SQL, the SELECT statement is arguably the most versatile / complex. -Baseline for efﬁciency is Linear Search - Start at the beginning of a list and proceed element by element until: - You ﬁnd what you’re looking for - You get to the last element and haven’t found it Searching -Record - A collection of values for attributes of a single entity instance; a row of a table -Collection - a set of records of the same entity type; a table - Trivially, stored in some sequential order like a list -Search Key - A value for an attribute from the entity type - Could be >= 1 attribute Lists of Records -If each record takes up x bytes of memory, then for n records, we need n*x bytes of memory. -Contiguously Allocated List - All n*x bytes are allocated as a single “chunk” of memory -Linked List - Each record needs x bytes + additional space for 1 or 2 memory addresses - Individual records are linked together in a type of chain using memory addresses Contiguous vs Linked 5 6 Records Contiguously Allocated - Array front back 6 Records Linked by memory addresses - Linked List Extra storage for a memory address Pros and Cons -Arrays are faster for random access, but slow for inserting anywhere but the end -Linked Lists are faster for inserting anywhere in the list, but slower for random access 6 Insert after 2nd record records: records: 5 records had to be moved to make space Insert after 2nd record Observations: - Arrays - fast for random access - slow for random insertions - Linked Lists - slow for random access - fast for random insertions Binary Search - Input: array of values in sorted order, target value - Output: the location (index) of where target is located or some value indicating target was not found def binary_search(arr, target) left, right = 0, len(arr) - 1 while left <= right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] < target: left = mid + 1 else: right = mid - 1 return -1 8 A C G M P R Z target = A mid Since target < arr[mid], we reset right to mid - 1. left right A C G M P R Z target = A mid left right Time Complexity -Linear Search - Best case: target is found at the ﬁrst element; only 1 comparison - Worst case: target is not in the array; n comparisons - Therefore, in the worst case, linear search is O(n) time complexity. -Binary Search - Best case: target is found at mid; 1 comparison (inside the loop) - Worst case: target is not in the array; log2 n comparisons - Therefore, in the worst case, binary search is O(log2n) time complexity. Back to Database Searching - Assume data is stored on disk by column id’s value - Searching for a speciﬁc id is fast. - But what if we want to search for a speciﬁc specialVal? - Only option is linear scan of that column - Can’t store data on disk sorted by both id and specialVal (at the same time) - data would have to be duplicated → space inefﬁcient Back to Database Searching - Assume data is stored on disk by column id’s value - Searching for a speciﬁc id = fast. - But what if we want to search for a speciﬁc specialVal? - Only option is linear scan of that column - Can’t store data on disk sorted by both id and specialVal (at the same time) - data would have to be duplicated → space inefﬁcient 11 We need an external data structure to support faster searching by specialVal than a linear scan. What do we have in our arsenal? 1) An array of tuples (specialVal, rowNumber) sorted by specialVal a) We could use Binary Search to quickly locate a particular specialVal and ﬁnd its corresponding row in the table b) But, every insert into the table would be like inserting into a sorted array - slow… 2) A linked list of tuples (specialVal, rowNumber) sorted by specialVal a) searching for a specialVal would be slow - linear scan required b) But inserting into the table would theoretically be quick to also add to the list. Something with Fast Insert and Fast Search? - Binary Search Tree - a binary tree where every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent. 13 Image from: https://courses.grainger.illinois.edu/cs225/sp2019/notes/bst/ To the Board!



========================================
Document: 03 - Moving Beyond the Relational Model.pdf
========================================

DS 4300 Moving Beyond the Relational Model Mark Fontenot, PhD Northeastern University Beneﬁts of the Relational Model - (Mostly) Standard Data Model and Query Language - ACID Compliance (more on this in a second) - Atomicity, Consistency, Isolation, Durability - Works well will highly structured data - Can handle large amounts of data - Well understood, lots of tooling, lots of experience Relational Database Performance Many ways that a RDBMS increases efﬁciency: - indexing (the topic we focused on) - directly controlling storage - column oriented storage vs row oriented storage - query optimization - caching/prefetching - materialized views - precompiled stored procedures - data replication and partitioning Transaction Processing - Transaction - a sequence of one or more of the CRUD operations performed as a single, logical unit of work - Either the entire sequence succeeds (COMMIT) - OR the entire sequence fails (ROLLBACK or ABORT) - Help ensure - Data Integrity - Error Recovery - Concurrency Control - Reliable Data Storage - Simpliﬁed Error Handling ACID Properties - Atomicity - transaction is treated as an atomic unit - it is fully executed or no parts of it are executed - Consistency - a transaction takes a database from one consistent state to another consistent state - consistent state - all data meets integrity constraints ACID Properties - Isolation - Two transactions T1 and T2 are being executed at the same time but cannot affect each other - If both T1 and T2 are reading the data - no problem - If T1 is reading the same data that T2 may be writing, can result in: - Dirty Read - Non-repeatable Read - Phantom Reads Isolation: Dirty Read 7 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Dirty Read - a transaction T1 is able to read a row that has been modiﬁed by another transaction T2 that hasn’t yet executed a COMMIT Isolation: Non-Repeatable Read 8 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Non-repeatable Read - two queries in a single transaction T1 execute a SELECT but get different values because another transaction T2 has changed data and COMMITTED Isolation: Phantom Reads 9 Figure from: https://www.mybluelinux.com/relational-databases-explained/ Phantom Reads - when a transaction T1 is running and another transaction T2 adds or deletes rows from the set T1 is using Example Transaction - Transfer $$ 10 DELIMITER // CREATE PROCEDURE transfer( IN sender_id INT, IN receiver_id INT, IN amount DECIMAL(10,2) ) BEGIN DECLARE rollback_message VARCHAR(255) DEFAULT 'Transaction rolled back: Insufficient funds'; DECLARE commit_message VARCHAR(255) DEFAULT 'Transaction committed successfully'; -- Start the transaction START TRANSACTION; -- Attempt to debit money from account 1 UPDATE accounts SET balance = balance - amount WHERE account_id = sender_id; -- Attempt to credit money to account 2 UPDATE accounts SET balance = balance + amount WHERE account_id = receiver_id; -- Continued Next Slide Example Transaction - Transfer $$ 11 -- Continued from previous slide -- Check if there are sufficient funds in account 1 -- Simulate a condition where there are insufficient funds IF (SELECT balance FROM accounts WHERE account_id = sender_id) < 0 THEN -- Roll back the transaction if there are insufficient funds ROLLBACK; SIGNAL SQLSTATE '45000' -- 45000 is unhandled, user-defined error SET MESSAGE_TEXT = rollback_message; ELSE -- Log the transactions if there are sufficient funds INSERT INTO transactions (account_id, amount, transaction_type) VALUES (sender_id, -amount, 'WITHDRAWAL'); INSERT INTO transactions (account_id, amount, transaction_type) VALUES (receiver_id, amount, 'DEPOSIT'); -- Commit the transaction COMMIT; SELECT commit_message AS 'Result'; END IF; END // DELIMITER ; ACID Properties - Durability - Once a transaction is completed and committed successfully, its changes are permanent. - Even in the event of a system failure, committed transactions are preserved - For more info on Transactions, see: - Kleppmann Book Chapter 7 But … Relational Databases may not be the solution to all problems… - sometimes, schemas evolve over time - not all apps may need the full strength of ACID compliance - joins can be expensive - a lot of data is semi-structured or unstructured (JSON, XML, etc) - Horizontal scaling presents challenges - some apps need something more performant (real time, low latency systems) Scalability - Up or Out? Conventional Wisdom: Scale vertically (up, with bigger, more powerful systems) until the demands of high-availability make it necessary to scale out with some type of distributed computing model But why? Scaling up is easier - no need to really modify your architecture. But there are practical and ﬁnancial limits However: There are modern systems that make horizontal scaling less problematic. So what? Distributed Data when Scaling Out A distributed system is “a collection of independent computers that appear to its users as one computer.” -Andrew Tennenbaum Characteristics of Distributed Systems: - computers operate concurrently - computers fail independently - no shared global clock Distributed Storage - 2 Directions 16 Single Main Node Distributed Data Stores - Data is stored on > 1 node, typically replicated - i.e. each block of data is available on N nodes - Distributed databases can be relational or non-relational - MySQL and PostgreSQL support replication and sharding - CockroachDB - new player on the scene - Many NoSQL systems support one or both models - But remember: Network partitioning is inevitable! - network failures, system failures - Overall system needs to be Partition Tolerant - System can keep running even w/ network partition The CAP Theorem The CAP Theorem 19 The CAP Theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: - Consistency - Every read receives the most recent write or error thrown - Availability - Every request receives a (non-error) response - but no guarantee that the response contains the most recent write - Partition Tolerance - The system can continue to operate despite arbitrary network issues. CAP Theorem - Database View 20 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID. CAP Theorem - Database View 21 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network issues - Consistency + Partition Tolerance: If system responds with data from a distributed store, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. CAP in Reality What it is really saying: - If you cannot limit the number of faults, requests can be directed to any server, and you insist on serving every request, then you cannot possibly be consistent. But it is interpreted as: - You must always give up something: consistency, availability, or tolerance to failure.



========================================
Document: 04 - Data Replication.pdf
========================================

DS 4300 Replicating Data Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks! Distributing Data - Beneﬁts 2 - Scalability / High throughput: Data volume or Read/Write load grows beyond the capacity of a single machine - Fault Tolerance / High Availability: Your application needs to continue working even if one or more machines goes down. - Latency: When you have users in different parts of the world you want to give them fast performance too Distributed Data - Challenges - Consistency: Updates must be propagated across the network. - Application Complexity: Responsibility for reading and writing data in a distributed environment often falls to the application. Vertical Scaling - Shared Memory Architectures - Geographically Centralized server - Some fault tolerance (via hot-swappable components) Vertical Scaling - Shared Disk Architectures - Machines are connected via a fast network - Contention and the overhead of locking limit scalability (high-write volumes) … BUT ok for Data Warehouse applications (high read volumes) AWS EC2 Pricing - Oct 2024 6 > $78,000/month https://aws.amazon.com/ec2/pricing/on-demand/ Horizontal Scaling - Shared Nothing Architectures -Each node has its own CPU, memory, and disk -Coordination via application layer using conventional network -Geographically distributed -Commodity hardware Data - Replication vs Partitioning 8 Replicates have same data as Main Partitions have a subset of the data Replication Common Strategies for Replication - Single leader model - Multiple leader model - Leaderless model Distributed databases usually adopt one of these strategies. Leader-Based Replication - All writes from clients go to the leader - Leader sends replication info to the followers - Followers process the instructions from the leader - Clients can read from either the leader or followers Leader-Based Replication 12 This write could NOT be sent to one of the followers… only the leader. Leader-Based Replication - Very Common Strategy Relational: - MySQL, - Oracle, - SQL Server, - PostgreSQL NoSQL: - MongoDB, - RethinkDB (realtime web apps), - Espresso (LinkedIn) Messaging Brokers: Kafka, RabbitMQ How Is Replication Info Transmitted to Followers? 14 Replication Method Description Statement-based Send INSERT, UPDATE, DELETEs to replica. Simple but error-prone due to non-deterministic functions like now(), trigger side-effects, and difﬁculty in handling concurrent transactions. Write-ahead Log (WAL) A byte-level speciﬁc log of every change to the database. Leader and all followers must implement the same storage engine and makes upgrades difﬁcult. Logical (row-based) Log For relational DBs: Inserted rows, modiﬁed rows (before and after), deleted rows. A transaction log will identify all the rows that changed in each transaction and how they changed. Logical logs are decoupled from the storage engine and easier to parse. Trigger-based Changes are logged to a separate table whenever a trigger ﬁres in response to an insert, update, or delete. Flexible because you can have application speciﬁc replication, but also more error prone. Synchronous vs Asynchronous Replication Synchronous: Leader waits for a response from the follower Asynchronous: Leader doesn’t wait for conﬁrmation. 15 Synchronous: Asynchronous: What Happens When the Leader Fails? Challenges: How do we pick a new Leader Node? -Consensus strategy – perhaps based on who has the most updates? -Use a controller node to appoint new leader? AND… how do we conﬁgure clients to start writing to the new leader? What Happens When the Leader Fails? More Challenges: - If asynchronous replication is used, new leader may not have all the writes How do we recover the lost writes? Or do we simply discard? - After (if?) the old leader recovers, how do we avoid having multiple leaders receiving conﬂicting data? (Split brain: no way to resolve conﬂicting requests. - Leader failure detection. Optimal timeout is tricky. Replication Lag refers to the time it takes for writes on the leader to be reﬂected on all of the followers. -Synchronous replication: Replication lag causes writes to be slower and the system to be more brittle as num followers increases. -Asynchronous replication: We maintain availability but at the cost of delayed or eventual consistency. This delay is called the inconsistency window. Replication Lag Read-after-Write Consistency Scenario - you’re adding a comment to a Reddit post… after you click Submit and are back at the main post, your comment should show up for you. - Less important for other users to see your comment as immediately. Implementing Read-After-Write Consistency Method 1: Modiﬁable data (from the client’s perspective) is always read from the leader. Implementing Read-After-Write Consistency Method 2: Dynamically switch to reading from leader for “recently updated” data. - For example, have a policy that all requests within one minute of last update come from leader. But… This Can Create Its Own Challenges 22 We created followers so they would be proximal to users. BUT… now we have to route requests to distant leaders when reading modiﬁable data?? :( Monotonic Read Consistency Monotonic read anomalies: occur when a user reads values out of order from multiple followers. Monotonic read consistency: ensures that when a user makes multiple reads, they will not read older data after previously reading newer data. Consistent Preﬁx Reads Reading data out of order can occur if different partitions replicate data at different rates. There is no global write consistency. Consistent Preﬁx Read Guarantee - ensures that if a sequence of writes happens in a certain order, anyone reading those writes will see them appear in the same order. 24 A B How far into the future can you see, Ms. B? About 10 seconds usually, Mr A.



========================================
Document: 05b - Redis in Docker.pdf
========================================

DS 4300 Redis in Docker Setup Mark Fontenot, PhD Northeastern University Pre-Requisites 2 - You have installed Docker Desktop - You have installed Jetbrains DataGrip Step 1 - Find the Redis Image - Open Docker Desktop - Use the Built In search to ﬁnd the Redis Image - Click Run Step 2 - Conﬁgure & Run the Container - Give the new container a name - Enter 6379 in Host Port ﬁeld - Click Run - Give Docker some time to download and start Redis Step 3 - Set up Data Source in DataGrip - Start DataGrip - Create a new Redis Data Source - You can use the + in the Database Explorer OR - You can use New from the File Menu Step 4 - Conﬁgure the Data Source - Give the data source a name - Install Drivers if needed (message above Test Connection) - Test the Connection to Redis - Click OK if connection test was successful 6 There will be a message to install drivers above Test Connection if they aren’t already installed



========================================
Document: 05 - NoSQL Intro + KV DBs.pdf
========================================

DS 4300 NoSQL & KV DBs Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks! Distributed DBs and ACID - Pessimistic Concurrency -ACID transactions - Focuses on “data safety” - considered a pessimistic concurrency model because it assumes one transaction has to protect itself from other transactions - IOW, it assumes that if something can go wrong, it will. - Conﬂicts are prevented by locking resources until a transaction is complete (there are both read and write locks) - Write Lock Analogy → borrowing a book from a library… If you have it, no one else can. 2 See https://www.freecodecamp.org/news/how-databases-guarantee-isolation for more for a deeper dive. Optimistic Concurrency -Transactions do not obtain locks on data when they read or write -Optimistic because it assumes conﬂicts are unlikely to occur - Even if there is a conﬂict, everything will still be OK. -But how? - Add last update timestamp and version number columns to every table… read them when changing. THEN, check at the end of transaction to see if any other transaction has caused them to be modiﬁed. Optimistic Concurrency -Low Conﬂict Systems (backups, analytical dbs, etc.) - Read heavy systems - the conﬂicts that arise can be handled by rolling back and re-running a transaction that notices a conﬂict. - So, optimistic concurrency works well - allows for higher concurrency -High Conﬂict Systems - rolling back and rerunning transactions that encounter a conﬂict → less efﬁcient - So, a locking scheme (pessimistic model) might be preferable NoSQL - “NoSQL” ﬁrst used in 1998 by Carlo Strozzi to describe his relational database system that did not use SQL. - More common, modern meaning is “Not Only SQL” - But, sometimes thought of as non-relational DBs - Idea originally developed, in part, as a response to processing unstructured web-based data. 5 https://www.dataversity.net/a-brief-history-of-non-relational-databases/ CAP Theorem Review 6 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ You can have 2, but not 3, of the following: - Consistency*: Every user of the DB has an identical view of the data at any given instant - Availability: In the event of a failure, the database system remains operational - Partition Tolerance: The database can maintain operations in the event of the network’s failing between two segments of the distributed system * Note, the definition of Consistency in CAP is different from that of ACID. CAP Theorem Review 7 Reference: https://alperenbayramoglu.com/posts/understanding-cap-theorem/ - Consistency + Availability: System always responds with the latest data and every request gets a response, but may not be able to deal with network partitions - Consistency + Partition Tolerance: If system responds with data from the distrib. system, it is always the latest, else data request is dropped. - Availability + Partition Tolerance: System always sends are responds based on distributed store, but may not be the absolute latest data. ACID Alternative for Distrib Systems - BASE -Basically Available -Guarantees the availability of the data (per CAP), but response can be “failure”/“unreliable” because the data is in an inconsistent or changing state -System appears to work most of the time ACID Alternative for Distrib Systems - BASE -Soft State - The state of the system could change over time, even w/o input. Changes could be result of eventual consistency. -Data stores don’t have to be write-consistent -Replicas don’t have to be mutually consistent ACID Alternative for Distrib Systems - BASE -Eventual Consistency - The system will eventually become consistent -All writes will eventually stop so all nodes/replicas can be updated Categories of NoSQL DBs - Review First Up → Key-Value Databases Key Value Stores key = value - Key-value stores are designed around: - simplicity - the data model is extremely simple - comparatively, tables in a RDBMS are very complex. - lends itself to simple CRUD ops and API creation Key Value Stores key = value - Key-value stores are designed around: - speed - usually deployed as in-memory DB - retrieving a value given its key is typically a O(1) op b/c hash tables or similar data structs used under the hood - no concept of complex queries or joins… they slow things down Key Value Stores key = value - Key-value stores are designed around: - scalability - Horizontal Scaling is simple - add more nodes - Typically concerned with eventual consistency, meaning in a distributed environment, the only guarantee is that all nodes will eventually converge on the same value. KV DS Use Cases - EDA/Experimentation Results Store - store intermediate results from data preprocessing and EDA - store experiment or testing (A/B) results w/o prod db - Feature Store - store frequently accessed feature → low-latency retrieval for model training and prediction - Model Monitoring - store key metrics about performance of model, for example, in real-time inferencing. KV SWE Use Cases - Storing Session Information - everything about the current session can be stored via a single PUT or POST and retrieved with a single GET …. VERY Fast - User Proﬁles & Preferences - User info could be obtained with a single GET operation… language, TZ, product or UI preferences - Shopping Cart Data - Cart data is tied to the user - needs to be available across browsers, machines, sessions - Caching Layer: - In front of a disk-based database Redis DB - Redis (Remote Directory Server) - Open source, in-memory database - Sometimes called a data structure store - Primarily a KV store, but can be used with other models: Graph, Spatial, Full Text Search, Vector, Time Series - From db-engines.com Ranking of KV Stores: Redis - It is considered an in-memory database system, but… - Supports durability of data by: a) essentially saving snapshots to disk at speciﬁc intervals or b) append-only ﬁle which is a journal of changes that can be used for roll-forward if there is a failure - Originally developed in 2009 in C++ - Can be very fast … > 100,000 SET ops / second - Rich collection of commands - Does NOT handle complex data. No secondary indexes. Only supports lookup by Key. Redis Data Types Keys: - usually strings but can be any binary sequence Values: - Strings - Lists (linked lists) - Sets (unique unsorted string elements) - Sorted Sets - Hashes (string → string) - Geospatial data Setting Up Redis in Docker - In Docker Desktop, search for Redis. - Pull/Run the latest image (see above) - Optional Settings: add 6379 to Ports to expose that port so we can connect to it. - Normally, you would not expose the Redis port for security reasons - If you did this in a prod environment, major security hole. - Notice, we didn’t set a password… Connecting from DataGrip - File > New > Data Source > Redis - Give the Data Source a Name - Make sure the port is 6379 - Test the connection ✅ Redis Database and Interaction - Redis provides 16 databases by default - They are numbered 0 to 15 - There is no other name associated - Direct interaction with Redis is through a set of commands related to setting and getting k/v pairs (and variations) - Many language libraries available as well. Foundation Data Type - String - Sequence of bytes - text, serialized objects, bin arrays - Simplest data type - Maps a string to another string - Use Cases: - caching frequently accessed HTML/CSS/JS fragments - conﬁg settings, user settings info, token management - counting web page/app screen views OR rate limiting Some Initial Basic Commands - SET /path/to/resource 0 SET user:1 “John Doe” GET /path/to/resource EXISTS user:1 DEL user:1 KEYS user* - SELECT 5 - select a different database Some Basic Commands - SET someValue 0 INCR someValue #increment by 1 INCRBY someValue 10 #increment by 10 DECR someValue #decrement by 1 DECRBY someValue 5 #decrement by 5 - INCR parses the value as int and increments (or adds to value) - SETNX key value - only sets value to key if key does not already exist Hash Type 27 - Value of KV entry is a collection of ﬁeld-value pairs - Use Cases: - Can be used to represent basic objects/structures - number of ﬁeld/value pairs per hash is 2^32-1 - practical limit: available system resources (e.g. memory) - Session information management - User/Event tracking (could include TTL) - Active Session Tracking (all sessions under one hash key) Hash Commands 28 HSET bike:1 model Demios brand Ergonom price 1971 HGET bike:1 model HGET bike:1 price HGETALL bike:1 HMGET bike:1 model price weight HINCRBY bike:1 price 100 What is returned? List Type - Value of KV Pair is linked lists of string values - Use Cases: - implementation of stacks and queues - queue management & message passing queues (producer/consumer model) - logging systems (easy to keep in chronological order) - build social media streams/feeds - message history in a chat application - batch processing by queueing up a set of tasks to be executed sequentially at a later time Linked Lists Crash Course - Sequential data structure of linked nodes (instead of contiguously allocated memory) - Each node points to the next element of the list (except the last one - points to nil/null) - O(1) to insert new value at front or insert new value at end 30 10 front back nil List Commands - Queue Queue-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 RPOP bikes:repairs RPOP biles:repairs List Commands - Stack Stack-like Ops LPUSH bikes:repairs bike:1 LPUSH bikes:repairs bike:2 LPOP bikes:repairs LPOP biles:repairs List Commands - Others Other List Ops LLEN mylist LRANGE <key> <start> <stop> LRANGE mylist 0 3 LRANGE mylist 0 0 LRANGE mylist -2 -1 33 LPUSH mylist “one” LPUSH mylist “two” LPUSH mylist “three” JSON Type - Full support of the JSON standard - Uses JSONPath syntax for parsing/navigating a JSON document - Internally, stored in binary in a tree-structure → fast access to sub elements Set Type - Unordered collection of unique strings (members) - Use Cases: - track unique items (IP addresses visiting a site, page, screen) - primitive relation (set of all students in DS4300) - access control lists for users and permission structures - social network friends lists and/or group membership - Supports set operations!! Set Commands SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SISMEMBER ds4300 “Mark” SISMEMBER ds4300 “Nick” SCARD ds4300 Set Commands SADD ds4300 “Mark” SADD ds4300 “Sam” SADD cs3200 “Nick” SADD cs3200 “Sam” SCARD ds4300 SINTER ds4300 cs3200 SDIFF ds4300 cs3200 SREM ds4300 “Mark” SRANDMEMBER ds4300



========================================
Document: 06 - Redis + Python.pdf
========================================

DS 4300 Redis + Python Mark Fontenot, PhD Northeastern University Redis-py 2 - Redis-py is the standard client for Python. - Maintained by the Redis Company itself - GitHub Repo: redis/redis-py - In your 4300 Conda Environment: pip install redis Connecting to the Server - For your Docker deployment, host could be localhost or 127.0.0.1 - Port is the port mapping given when you created the container (probably the default 6379) - db is the database 0-15 you want to connect to - decode_responses → data comes back from server as bytes. Setting this true converter them (decodes) to strings. 3 import redis redis_client = redis.Redis(host=’localhost’, port=6379, db=2, decode_responses=True) Redis Command List - Full List > here < - Use Filter to get to command for the particular data structure you’re targeting (list, hash, set, etc.) - Redis.py Documentation > here < - The next slides are not meant to be an exhaustive list of commands, only some highlights. Check the documentation for a complete list. String Commands # r represents the Redis client object r.set(‘clickCount:/abc’, 0) val = r.get(‘clickCount:/abc’) r.incr(‘clickCount:/abc’) ret_val = r.get(‘clickCount:/abc’) print(f’click count = {ret_val}’) String Commands - 2 # r represents the Redis client object redis_client.mset({'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}) print(redis_client.mget('key1', 'key2', 'key3')) # returns as list [‘val1’, ‘val2’, ‘val3’] String Commands - 3 - set(), mset(), setex(), msetnx(), setnx() - get(), mget(), getex(), getdel() - incr(), decr(), incrby(), decrby() - strlen(), append() List Commands - 1 # create list: key = ‘names’ # values = [‘mark’, ‘sam’, ‘nick’] redis_client.rpush('names', 'mark', 'sam', 'nick') # prints [‘mark’, ‘sam’, ‘nick’] print(redis_client.lrange('names', 0, -1)) List Commands - 2 - lpush(), lpop(), lset(), lrem() - rpush(), rpop() - lrange(), llen(), lpos() - Other commands include moving elements between lists, popping from multiple lists at the same time, etc. Hash Commands - 1 redis_client.hset('user-session:123', mapping={'first': 'Sam', 'last': 'Uelle', 'company': 'Redis', 'age': 30 }) # prints: #{'name': 'Sam', 'surname': 'Uelle', 'company': 'Redis', 'age': '30'} print(redis_client.hgetall('user-session:123')) Hash Commands - 2 - hset(), hget(), hgetall() - hkeys() - hdel(), hexists(), hlen(), hstrlen() Redis Pipelines - Helps avoid multiple related calls to the server → less network overhead 12 r = redis.Redis(decode_responses=True) pipe = r.pipeline() for i in range(5): pipe.set(f"seat:{i}", f"#{i}") set_5_result = pipe.execute() print(set_5_result) # >>> [True, True, True, True, True] pipe = r.pipeline() # "Chain" pipeline commands together. get_3_result = pipe.get("seat:0").get("seat:3").get("seat:4").execute() print(get_3_result) # >>> ['#0', '#3', '#4'] Redis in Context Redis in ML - Simpliﬁed Example 14 Source: https://www.featureform.com/post/feature-stores-explained-the-three-common-architectures Redis in DS/ML 15 Source: https://madewithml.com/courses/mlops/feature-store/



========================================
Document: 07 - Document DBs and Mongo.pdf
========================================

DS 4300 Document Databases & MongoDB Mark Fontenot, PhD Northeastern University Some material used with permission from Dr. Rachlin, with thanks! Document Database A Document Database is a non-relational database that stores data as structured documents, usually in JSON. They are designed to be simple, ﬂexible, and scalable. What is JSON? - JSON (JavaScript Object Notation) - a lightweight data-interchange format - It is easy for humans to read and write. - It is easy for machines to parse and generate. - JSON is built on two structures: - A collection of name/value pairs. In various languages, this is operationalized as an object, record, struct, dictionary, hash table, keyed list, or associative array. - An ordered list of values. In most languages, this is operationalized as an array, vector, list, or sequence. - These are two universal data structures supported by virtually all modern programming languages - Thus, JSON makes a great data interchange format. JSON Syntax 4 https://www.json.org/json-en.html Binary JSON? BSON - BSON → Binary JSON - binary-encoded serialization of a JSON-like document structure - supports extended types not part of basic JSON (e.g. Date, BinaryData, etc) - Lightweight - keep space overhead to a minimum - Traversable - designed to be easily traversed, which is vitally important to a document DB - Efﬁcient - encoding and decoding must be efﬁcient - Supported by many modern programming languages XML (eXtensible Markup Language) -Precursor to JSON as data exchange format -XML + CSS → web pages that separated content and formatting -Structurally similar to HTML, but tag set is extensible XML-Related Tools/Technologies - Xpath - a syntax for retrieving speciﬁc elements from an XML doc - Xquery - a query language for interrogating XML documents; the SQL of XML - DTD - Document Type Deﬁnition - a language for describing the allowed structure of an XML document - XSLT - eXtensible Stylesheet Language Transformation - tool to transform XML into other formats, including non-XML formats such as HTML. Why Document Databases? - Document databases address the impedance mismatch problem between object persistence in OO systems and how relational DBs structure data. - OO Programming → Inheritance and Composition of types. - How do we save a complex object to a relational database? We basically have to deconstruct it. - The structure of a document is self-describing. - They are well-aligned with apps that use JSON/XML as a transport layer MongoDB MongoDB - Started in 2007 after Doubleclick was acquired by Google, and 3 of its veterans realized the limitations of relational databases for serving > 400,000 ads per second - MongoDB was short for Humongous Database - MongoDB Atlas released in 2016 → documentdb as a service 10 https://www.mongodb.com/company/our-story MongoDB Structure 11 Database Collection A Collection B Collection C Document 1 Document 2 Document 3 Document 1 Document 2 Document 3 Document 1 Document 2 Document 3 MongoDB Documents - No predeﬁned schema for documents is needed - Every document in a collection could have different data/schema Relational vs Mongo/Document DB 13 RDBMS MongoDB Database Database Table/View Collection Row Document Column Field Index Index Join Embedded Document Foreign Key Reference MongoDB Features - Rich Query Support - robust support for all CRUD ops - Indexing - supports primary and secondary indices on document ﬁelds - Replication - supports replica sets with automatic failover - Load balancing built in MongoDB Versions -MongoDB Atlas - Fully managed MongoDB service in the cloud (DBaaS) -MongoDB Enterprise - Subscription-based, self-managed version of MongoDB -MongoDB Community - source-available, free-to-use, self-managed Interacting with MongoDB -mongosh → MongoDB Shell - CLI tool for interacting with a MongoDB instance -MongoDB Compass - free, open-source GUI to work with a MongoDB database -DataGrip and other 3rd Party Tools -Every major language has a library to interface with MongoDB - PyMongo (Python), Mongoose (JavaScript/node), … Mongodb Community Edition in Docker - Create a container - Map host:container port 27017 - Give initial username and password for superuser 17 E D MongoDB Compass - GUI Tool for interacting with MongoDB instance - Download and install from > here <. Load MFlix Sample Data Set - In Compass, create a new Database named mﬂix - Download mﬂix sample dataset and unzip it - Import JSON ﬁles for users, theaters, movies, and comments into new collections in the mﬂix database Creating a Database and Collection 20 mﬂix users To Create a new DB: To Create a new Collection: mongosh - Mongo Shell - ﬁnd(...) is like SELECT 21 collection.find({ ____ }, { ____ }) ﬁlters projections mongosh - ﬁnd() - SELECT * FROM users; 22 use mflix db.users.find() mongosh - ﬁnd() - SELECT * FROM years WHERE name = “Davos Seaworth”; 23 db.users.find({"name": "Davos Seaworth"}) ﬁlter mongosh - ﬁnd() - SELECT * FROM movies WHERE rated in ("PG", "PG-13") 24 db.movies.find({rated: {$in:[ "PG", "PG-13" ]}}) mongosh - ﬁnd() - Return movies which were released in Mexico and have an IMDB rating of at least 7 25 db.movies.find( { "countries": "Mexico", "imdb.rating": { $gte: 7 } } ) mongosh - ﬁnd() - Return movies from the movies collection which were released in 2010 and either won at least 5 awards or have a genre of Drama 26 db.movies.find( { “year”: 2010, $or: [ { "awards.wins": { $gte: 5 } }, { “genres”: "Drama" } ] }) Comparison Operators mongosh - countDocuments() - How many movies from the movies collection were released in 2010 and either won at least 5 awards or have a genre of Drama 28 db.movies.countDocuments( { “year”: 2010, $or: [ { "awards.wins": { $gte: 5 } }, { “genres”: "Drama" } ] }) mongosh - project - Return the names of all movies from the movies collection that were released in 2010 and either won at least 5 awards or have a genre of Drama 29 db.movies.countDocuments( { “year”: 2010, $or: [ { "awards.wins": { $gte: 5 } }, { “genres”: "Drama" } ] }, {“name”: 1, “_id”: 0} ) 1 = return; 0 = don’t return PyMongo PyMongo -PyMongo is a Python library for interfacing with MongoDB instances 31 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) Getting a Database and Collection 32 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] collection = db[‘myCollection’] Inserting a Single Document 33 db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id) Count Documents in Collection - SELECT count(*) FROM collection 34 demodb.collection.count_documents({})



========================================
Document: 08 - PyMongo.pdf
========================================

DS 4300 MongoDB + PyMongo Mark Fontenot, PhD Northeastern University PyMongo -PyMongo is a Python library for interfacing with MongoDB instances 2 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) Getting a Database and Collection 3 from pymongo import MongoClient client = MongoClient( ‘mongodb://user_name:pw@localhost:27017’ ) db = client[‘ds4300’] # or client.ds4300 collection = db[‘myCollection’] #or db.myCollection Inserting a Single Document 4 db = client[‘ds4300’] collection = db[‘myCollection’] post = { “author”: “Mark”, “text”: “MongoDB is Cool!”, “tags”: [“mongodb”, “python”] } post_id = collection.insert_one(post).inserted_id print(post_id) Find all Movies from 2000 5 from bson.json_util import dumps # Find all movies released in 2000 movies_2000 = db.movies.find({"year": 2000}) # Print results print(dumps(movies_2000, indent = 2)) Jupyter Time - Activate your DS4300 conda or venv python environment - Install pymongo with pip install pymongo - Install Jupyter Lab in you python environment - pip install jupyterlab - Download and unzip > this < zip ﬁle - contains 2 Jupyter Notebooks - In terminal, navigate to the folder where you unzipped the ﬁles, and run jupyter lab



========================================
Document: 09 - Introduction to Graph Data Model.pdf
========================================

DS 4300 Introduction to the Graph Data Model Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019) What is a Graph Database - Data model based on the graph data structure - Composed of nodes and edges - edges connect nodes - each is uniquely identiﬁed - each can contain properties (e.g. name, occupation, etc) - supports queries based on graph-oriented operations - traversals - shortest path - lots of others Where do Graphs Show up? - Social Networks - yes… things like Instagram, - but also… modeling social interactions in ﬁelds like psychology and sociology - The Web - it is just a big graph of “pages” (nodes) connected by hyperlinks (edges) - Chemical and biological data - systems biology, genetics, etc. - interaction relationships in chemistry Basics of Graphs and Graph Theory What is a graph? Labeled Property Graph - Composed of a set of node (vertex) objects and relationship (edge) objects - Labels are used to mark a node as part of a group - Properties are attributes (think KV pairs) and can exist on nodes and relationships - Nodes with no associated relationships are OK. Edges not connected to nodes are not permitted. Example 2 Labels: - person - car 4 relationship types: - Drives - Owns - Lives_with - Married_to Properties Paths A path is an ordered sequence of nodes connected by edges in which no nodes or edges are repeated. 7 1 2 3 6 5 4 Ex: 1 → 2 → 6 → 5 Not a path: 1 → 2 → 6 → 2 → 3 Flavors of Graphs Connected (vs. Disconnected) – there is a path between any two nodes in the graph Weighted (vs. Unweighted) – edge has a weight property (important for some algorithms) Directed (vs. Undirected) – relationships (edges) deﬁne a start and end node Acyclic (vs. Cyclic) – Graph contains no cycles Connected vs. Disconnected Weighted vs. Unweighted Directed vs. Undirected Cyclic vs Acyclic Sparse vs. Dense Trees Types of Graph Algorithms - Pathﬁnding - Pathﬁnding - ﬁnding the shortest path between two nodes, if one exists, is probably the most common operation - “shortest” means fewest edges or lowest weight - Average Shortest Path can be used to monitor efﬁciency and resiliency of networks. - Minimum spanning tree, cycle detection, max/min ﬂow… are other types of pathﬁnding BFS vs DFS Shortest Path Types of Graph Algorithms - Centrality & Community Detection - Centrality - determining which nodes are “more important” in a network compared to other nodes - EX: Social Network Inﬂuencers? - Community Detection - evaluate clustering or partitioning of nodes of a graph and tendency to strengthen or break apart Centrality Some Famous Graph Algorithms - Dijkstra’s Algorithm - single-source shortest path algo for positively weighted graphs - A* Algorithm - Similar to Dijkstra’s with added feature of using a heuristic to guide traversal - PageRank - measures the importance of each node within a graph based on the number of incoming relationships and the importance of the nodes from those incoming relationships Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune



========================================
Document: 10 - Neo4j.pdf
========================================

DS 4300 Neo4j Mark Fontenot, PhD Northeastern University Material referenced from Graph Algorithms - Practical Examples in Apache Spark and Neo4j by Needham and Hodler (O’Reilly Press, 2019) Neo4j - A Graph Database System that supports both transactional and analytical processing of graph-based data - Relatively new class of no-sql DBs - Considered schema optional (one can be imposed) - Supports various types of indexing - ACID compliant - Supports distributed computing - Similar: Microsoft CosmoDB, Amazon Neptune Neo4j - Query Language and Plugins - Cypher - Neo4j’s graph query language created in 2011 - Goal: SQL-equivalent language for graph databases - Provides a visual way of matching patterns and relationships (nodes)-[:CONNECT_TO]->(otherNodes) - APOC Plugin - Awesome Procedures on Cypher - Add-on library that provides hundreds of procedures and functions - Graph Data Science Plugin - provides efﬁcient implementations of common graph algorithms (like the ones we talked about yesterday) Neo4j in Docker Compose Docker Compose 5 -Supports multi-container management. -Set-up is declarative - using YAML docker-compose.yaml ﬁle - services - volumes - networks, etc. -1 command can be used to start, stop, or scale a number of services at one time. -Provides a consistent method for producing an identical environment (no more “well… it works on my machine!) -Interaction is mostly via command line docker-compose.yaml 6 services: neo4j: container_name: neo4j image: neo4j:latest ports: - 7474:7474 - 7687:7687 environment: - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD} - NEO4J_apoc_export_file_enabled=true - NEO4J_apoc_import_file_enabled=true - NEO4J_apoc_import_file_use__neo4j__config=true - NEO4J_PLUGINS=["apoc", "graph-data-science"] volumes: - ./neo4j_db/data:/data - ./neo4j_db/logs:/logs - ./neo4j_db/import:/var/lib/neo4j/import - ./neo4j_db/plugins:/plugins Never put “secrets” in a docker compose ﬁle. Use .env ﬁles. .env Files - .env ﬁles - stores a collection of environment variables - good way to keep environment variables for different platforms separate - .env.local - .env.dev - .env.prod 7 NEO4J_PASSWORD=abc123!!! .env file Docker Compose Commands -To test if you have Docker CLI properly installed, run: docker --version -Major Docker Commands - docker compose up - docker compose up -d - docker compose down - docker compose start - docker compose stop - docker compose build - docker compose build --no-cache localhost:7474 Neo4j Browser 10 https://neo4j.com/docs/browser-manual/current/visual-tour/ localhost:7474 Then login. Inserting Data by Creating Nodes CREATE (:User {name: "Alice", birthPlace: "Paris"}) CREATE (:User {name: "Bob", birthPlace: "London"}) CREATE (:User {name: "Carol", birthPlace: "London"}) CREATE (:User {name: "Dave", birthPlace: "London"}) CREATE (:User {name: "Eve", birthPlace: "Rome"}) Adding an Edge with No Variable Names CREATE (:User {name: "Alice", birthPlace: "Paris"}) CREATE (:User {name: "Bob", birthPlace: "London"}) MATCH (alice:User {name:”Alice”}) MATCH (bob:User {name: “Bob”}) CREATE (alice)-[:KNOWS {since: “2022-12-01”}]->(bob) 12 Note: Relationships are directed in neo4j. Matching Which users were born in London? MATCH (usr:User {birthPlace: “London”}) RETURN usr.name, usr.birthPlace Download Dataset and Move to Import Folder Clone this repo: https://github.com/PacktPublishing/Graph-Data-Science-with-Neo4j In Chapter02/data of data repo, unzip the netﬂix.zip ﬁle Copy netﬂix_titles.csv into the following folder where you put your docker compose ﬁle neo4j_db/neo4j_db/import Importing Data Basic Data Importing LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line CREATE(:Movie { id: line.show_id, title: line.title, releaseYear: line.release_year } ) 16 Type the following into the Cypher Editor in Neo4j Browser Loading CSVs - General Syntax LOAD CSV [WITH HEADERS] FROM 'file:///file_in_import_folder.csv' AS line [FIELDTERMINATOR ','] // do stuffs with 'line' Importing with Directors this Time LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, ",") as directors_list UNWIND directors_list AS director_name CREATE (:Person {name: trim(director_name)}) But this generates duplicate Person nodes (a director can direct more than 1 movie) Importing with Directors Merged MATCH (p:Person) DELETE p LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line WITH split(line.director, ",") as directors_list UNWIND directors_list AS director_name MERGE (:Person {name: director_name}) Adding Edges LOAD CSV WITH HEADERS FROM 'file:///netflix_titles.csv' AS line MATCH (m:Movie {id: line.show_id}) WITH m, split(line.director, ",") as directors_list UNWIND directors_list AS director_name MATCH (p:Person {name: director_name}) CREATE (p)-[:DIRECTED]->(m) Gut Check Let’s check the movie titled Ray: MATCH (m:Movie {title: "Ray"})<-[:DIRECTED]-(p:Person) RETURN m, p

